{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Piano Playalong Generation from MIDI\n",
    "\n",
    "## Description\n",
    "\n",
    "Add Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "We use the MIDITok Tokenizer to create Tokens from our MIDI files:\n",
    "\n",
    "https://miditok.readthedocs.io/\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, ConcatDataset, DataLoader, random_split\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "import lightning as L\n",
    "import partitura as pt\n",
    "\n",
    "from miditok import Structured, TokenizerConfig\n",
    "from miditok.pytorch_data import DatasetMIDI, DataCollator, split_midis_for_training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Tokenizer\n",
    "Using the \"Structured\"-Tokenizer from MidiTok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "TOKENIZER_PARAMS = {\n",
    "    \"pitch_range\": (21, 109),\n",
    "    \"beat_res\": {(0, 4): 8, (4, 12): 4},\n",
    "    \"num_velocities\": 32,\n",
    "    \"special_tokens\": [\"PAD\", \"BOS\", \"EOS\", \"MASK\"],\n",
    "    \"use_chords\": False,\n",
    "    \"use_rests\": False,\n",
    "    \"use_tempos\": False,\n",
    "    \"use_time_signatures\": False,\n",
    "    \"use_programs\": False,\n",
    "    \"num_tempos\": 32,  # number of tempo bins\n",
    "    \"tempo_range\": (40, 200),  # (min, max)\n",
    "}\n",
    "# Set to True if you want to use BPE\n",
    "USE_BPE = False\n",
    "\n",
    "config = TokenizerConfig(**TOKENIZER_PARAMS)\n",
    "\n",
    "# Creates the tokenizer\n",
    "tokenizer = Structured(config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data and tokenize\n",
    "Here you can filter out Midi-files you want to exclude (e.g. Tuning Tracks - Use lookup.json file to inspect data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1260 valid files, 55 invalid files excluded.\n"
     ]
    }
   ],
   "source": [
    "midi_paths = list(Path(\"data\").glob(\"**/*.mid\"))\n",
    "lookup_path = Path(\"data\", \"lookup.json\")\n",
    "\n",
    "# files to exclude:\n",
    "idx_del = []\n",
    "lookup = json.loads(lookup_path.read_text())\n",
    "midi_paths_cleaned = [] \n",
    "# files containing this string will be excluded\n",
    "lookup_str = \"tuning\"\n",
    "for key, title in lookup.items():\n",
    "    if lookup_str in title.lower():\n",
    "        idx_del.append(key)\n",
    "    # manually specify keys to exclude \n",
    "    elif str(key) in [\"0021\",\"0361\",\"0362\"]:   # verbal instructions and whole cds that couldnt be converted to midi (too long)\n",
    "        idx_del.append(key)\n",
    "    else:\n",
    "        midi_paths_cleaned.append(Path(\"data\", str(key)+\".mid\"))\n",
    "\n",
    "print(f\"Loaded {len(midi_paths_cleaned)} valid files, {len(idx_del)} invalid files excluded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer_path = Path(\"data\", \"tokenizer\", \"tokenizer.json\")\n",
    "\n",
    "# load tokenizer if it already exists\n",
    "if os.path.exists(tokenizer_path):\n",
    "    tokenizer = Structured(params=tokenizer_path)\n",
    "\n",
    "else:\n",
    "\n",
    "    \n",
    "    # Builds the vocabulary with BPE\n",
    "    if USE_BPE:\n",
    "        print(f\"Learning BPE...\")\n",
    "        tokenizer.learn_bpe(vocab_size=30000, files_paths=midi_paths_cleaned)\n",
    "        print(f\"Saving tokenizer with BPE to {tokenizer_path}\")\n",
    "        tokenizer.save_params(tokenizer_path)\n",
    "        print(\"Finished.\")\n",
    "    # Saves tokenizer without BPE\n",
    "    else:\n",
    "        print(f\"Saving tokenizer to {tokenizer_path}\")\n",
    "        tokenizer.save_params(tokenizer_path)\n",
    "        print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('data/0001.mid')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "midi_paths_cleaned[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of notes in the MIDI file: 2188\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(219.03645, 3.1432292, 168220, 2414, 71, 28, 0, 0, 'n2187')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from partitura import load_performance_midi\n",
    "score = load_performance_midi(midi_paths_cleaned[0])\n",
    "# Get the duration of the MIDI file in seconds\n",
    "\n",
    "print(\"Number of notes in the MIDI file:\", len(score.note_array()))\n",
    "score.note_array()[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8752"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "midi = tokenizer(midi_paths_cleaned[0])\n",
    "len(midi[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split MIDIs into subsequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split MIDIs into smaller chunks for training\n",
    "MAX_SEQUENCE_LENGTH = 128\n",
    "dataset_chunks_dir = Path(\"data\", \"midi_chunks\")\n",
    "\n",
    "if not os.path.exists(dataset_chunks_dir) or not os.listdir(dataset_chunks_dir):\n",
    "    midi_paths_chunks = split_midis_for_training(\n",
    "        files_paths=midi_paths_cleaned,\n",
    "        tokenizer=tokenizer,\n",
    "        save_dir=dataset_chunks_dir,\n",
    "        max_seq_len=MAX_SEQUENCE_LENGTH,\n",
    "    )\n",
    "else: \n",
    "    midi_paths_chunks = sorted([Path(p) for p in dataset_chunks_dir.iterdir() if p.is_file()])[1:] #first object is some hidden file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of files after splitting into chunks:  148667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PosixPath('data/midi_chunks/0001_0.mid'),\n",
       " PosixPath('data/midi_chunks/0001_1.mid'),\n",
       " PosixPath('data/midi_chunks/0001_10.mid'),\n",
       " PosixPath('data/midi_chunks/0001_11.mid'),\n",
       " PosixPath('data/midi_chunks/0001_12.mid'),\n",
       " PosixPath('data/midi_chunks/0001_13.mid'),\n",
       " PosixPath('data/midi_chunks/0001_14.mid'),\n",
       " PosixPath('data/midi_chunks/0001_15.mid'),\n",
       " PosixPath('data/midi_chunks/0001_16.mid'),\n",
       " PosixPath('data/midi_chunks/0001_17.mid')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Total number of files after splitting into chunks: \",len(os.listdir(dataset_chunks_dir)))\n",
    "midi_paths_chunks[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloading and Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataloader created.\n",
      "N samples in train/val : 1859 / 465\n"
     ]
    }
   ],
   "source": [
    "# Load midi chunks into dataset\n",
    "dataset = DatasetMIDI(\n",
    "    files_paths=midi_paths_chunks,\n",
    "    max_seq_len=MAX_SEQUENCE_LENGTH,\n",
    "    tokenizer=tokenizer,\n",
    "    bos_token_id=tokenizer[\"BOS_None\"],\n",
    "    eos_token_id=tokenizer[\"EOS_None\"]\n",
    ")\n",
    "dataset_train, dataset_val = random_split(dataset, [0.8, 0.2])\n",
    "\n",
    "# set PAD, BOS, EOS for collator - add attention masks\n",
    "# and pads (on left for generation - models usually cant generate from padding tokens)\n",
    "## TODO : does this make sense?\n",
    "collator = DataCollator(\n",
    "    tokenizer[\"PAD_None\"], pad_on_left=True, )\n",
    "\n",
    "# Set up dataloader\n",
    "data_loader_train = DataLoader(dataset=dataset_train, batch_size=64, collate_fn=collator,shuffle=True)\n",
    "data_loader_val = DataLoader(dataset=dataset_val, batch_size=64, collate_fn=collator,shuffle=False)\n",
    "print(f\"Dataloader created.\")\n",
    "print(f\"N samples in train/val : {len(data_loader_train)} / {len(data_loader_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[193,  38, 112,  ...,  56, 108, 125],\n",
       "         [189,  43, 113,  ...,  52, 111, 126],\n",
       "         [192,  49, 103,  ...,  50, 108, 128],\n",
       "         ...,\n",
       "         [190,  61, 106,  ...,  42, 107, 125],\n",
       "         [192,  50, 108,  ...,  52, 110, 124],\n",
       "         [198,  62, 109,  ...,  59, 105, 129]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1]], dtype=torch.int32)}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect elements in batch\n",
    "first_batch = next(iter(data_loader_train))\n",
    "first_batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from torch.nn import functional as F\n",
    "from transformers import GPT2LMHeadModel, AutoConfig, Trainer, TrainingArguments\n",
    "from transformers.optimization import AdamW \n",
    "\n",
    "# Setting the seed\n",
    "pl.seed_everything(42)\n",
    "\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "\n",
    "#for i, batch in enumerate(tqdm(data_loader)):\n",
    "#    pass\n",
    "    #print(f\"Training model on batch {i}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Number of Parameters in model:',\n",
       " 86083584,\n",
       " GPT2Config {\n",
       "   \"_name_or_path\": \"gpt2\",\n",
       "   \"activation_function\": \"gelu_new\",\n",
       "   \"architectures\": [\n",
       "     \"GPT2LMHeadModel\"\n",
       "   ],\n",
       "   \"attn_pdrop\": 0.1,\n",
       "   \"bos_token_id\": 1,\n",
       "   \"embd_pdrop\": 0.1,\n",
       "   \"eos_token_id\": 2,\n",
       "   \"initializer_range\": 0.02,\n",
       "   \"layer_norm_epsilon\": 1e-05,\n",
       "   \"model_type\": \"gpt2\",\n",
       "   \"n_ctx\": 128,\n",
       "   \"n_embd\": 768,\n",
       "   \"n_head\": 12,\n",
       "   \"n_inner\": null,\n",
       "   \"n_layer\": 12,\n",
       "   \"n_positions\": 1024,\n",
       "   \"reorder_and_upcast_attn\": false,\n",
       "   \"resid_pdrop\": 0.1,\n",
       "   \"scale_attn_by_inverse_layer_idx\": false,\n",
       "   \"scale_attn_weights\": true,\n",
       "   \"summary_activation\": null,\n",
       "   \"summary_first_dropout\": 0.1,\n",
       "   \"summary_proj_to_labels\": true,\n",
       "   \"summary_type\": \"cls_index\",\n",
       "   \"summary_use_proj\": true,\n",
       "   \"task_specific_params\": {\n",
       "     \"text-generation\": {\n",
       "       \"do_sample\": true,\n",
       "       \"max_length\": 50\n",
       "     }\n",
       "   },\n",
       "   \"transformers_version\": \"4.39.3\",\n",
       "   \"use_cache\": true,\n",
       "   \"vocab_size\": 314\n",
       " })"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load config and setup untrained model of gpt2\n",
    "config = AutoConfig.from_pretrained(\"gpt2\",vocab_size = len(tokenizer),\n",
    "                                    n_ctx=MAX_SEQUENCE_LENGTH, \n",
    "                                    bos_token_id = tokenizer[\"BOS_None\"], \n",
    "                                    eos_token_id = tokenizer[\"EOS_None\"],\n",
    "                                    )\n",
    "\n",
    "model = GPT2LMHeadModel(config)\n",
    "\"Number of Parameters in model:\", sum(p.numel() for p in model.parameters()),model.config\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3318070914.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[31], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    training_args = TrainingArguments{\u001b[0m\n\u001b[0m                                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir = \"./model/gpt-2\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    auto_find_batch_size=True,\n",
    "    num_train_epochs=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    weight_decay=0.1,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    learning_rate=5e-4,\n",
    "    fp16=True,\n",
    "    logging_steps=10,\n",
    "    )\n",
    "trainer = Trainer(\n",
    "    model=model, \n",
    "    tokenizer= tokenizer, \n",
    "    args = training_args,\n",
    "    data_collator=collator,\n",
    "    train_dataset=dataset_train,\n",
    "    eval_dataset=dataset_val,    \n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class BartModel(pl.LightningModule):\n",
    "    def __init__(self, learning_rate=1e-5):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.model = BartForConditionalGeneration(BartConfig())\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x).logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \n",
    "        loss = self.model(**batch).loss\n",
    "        \n",
    "        self.log('train_loss', loss, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self.model(**batch).loss\n",
    "        self.log('valid_loss', loss, on_step=True, sync_dist=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.parameters(),\n",
    "                          self.hparams.learning_rate)\n",
    "        return optimizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "`model` must be a `LightningModule` or `torch._dynamo.OptimizedModule`, got `BartModel`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[92], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m pl_model \u001b[38;5;241m=\u001b[39m BartModel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacebook/bart-base\u001b[39m\u001b[38;5;124m\"\u001b[39m,tokenizer)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/bac/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:538\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\n\u001b[1;32m    505\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    506\u001b[0m     model: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpl.LightningModule\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    510\u001b[0m     ckpt_path: Optional[_PATH] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    511\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    512\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Runs the full optimization routine.\u001b[39;00m\n\u001b[1;32m    513\u001b[0m \n\u001b[1;32m    514\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    536\u001b[0m \n\u001b[1;32m    537\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 538\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43m_maybe_unwrap_optimized\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    539\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m_lightning_module \u001b[38;5;241m=\u001b[39m model\n\u001b[1;32m    540\u001b[0m     _verify_strategy_supports_compile(model, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy)\n",
      "File \u001b[0;32m~/miniconda3/envs/bac/lib/python3.11/site-packages/pytorch_lightning/utilities/compile.py:132\u001b[0m, in \u001b[0;36m_maybe_unwrap_optimized\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[1;32m    131\u001b[0m _check_mixed_imports(model)\n\u001b[0;32m--> 132\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`model` must be a `LightningModule` or `torch._dynamo.OptimizedModule`, got `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(model)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    134\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: `model` must be a `LightningModule` or `torch._dynamo.OptimizedModule`, got `BartModel`"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[72], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/bac/lib/python3.11/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/bac/lib/python3.11/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/bac/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/bac/lib/python3.11/site-packages/miditok/pytorch_data/collators.py:92\u001b[0m, in \u001b[0;36mDataCollator.__call__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshift_labels:\n\u001b[1;32m     91\u001b[0m     x \u001b[38;5;241m=\u001b[39m x[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m---> 92\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     93\u001b[0m         y \u001b[38;5;241m=\u001b[39m y[:, \u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "next(iter(data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "practical",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
