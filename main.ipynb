{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Piano Playalong Generation from MIDI\n",
    "\n",
    "## Description\n",
    "\n",
    "Add Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "We use the MIDITok Tokenizer to create Tokens from our MIDI files:\n",
    "\n",
    "https://miditok.readthedocs.io/\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, ConcatDataset, DataLoader\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "import partitura as pt\n",
    "\n",
    "from miditok import Structured, TokenizerConfig\n",
    "from miditok.pytorch_data import DatasetMIDI, DataCollator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Tokenizer\n",
    "Using the \"Structured\"-Tokenizer from MidiTok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "TOKENIZER_PARAMS = {\n",
    "    \"pitch_range\": (21, 109),\n",
    "    \"beat_res\": {(0, 4): 8, (4, 12): 4},\n",
    "    \"num_velocities\": 32,\n",
    "    \"special_tokens\": [\"PAD\", \"BOS\", \"EOS\", \"MASK\"],\n",
    "    \"use_chords\": False,\n",
    "    \"use_rests\": False,\n",
    "    \"use_tempos\": False,\n",
    "    \"use_time_signatures\": False,\n",
    "    \"use_programs\": False,\n",
    "    \"num_tempos\": 32,  # number of tempo bins\n",
    "    \"tempo_range\": (40, 200),  # (min, max)\n",
    "}\n",
    "config = TokenizerConfig(**TOKENIZER_PARAMS)\n",
    "\n",
    "# Creates the tokenizer\n",
    "tokenizer = Structured(config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data and tokenize\n",
    "Here you can filter out Midi-files you want to exclude (e.g. Tuning Tracks - Use lookup.json file to inspect data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excluded 55 files, 1260 remaining.\n",
      "Learning BPE...\n",
      "\n",
      "\n",
      "\n",
      "Saving tokenizer to data/tokenizer/tokenizer.json\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "midi_paths = list(Path(\"data\").glob(\"**/*.mid\"))\n",
    "lookup_path = Path(\"data\", \"lookup.json\")\n",
    "tokenizer_path = Path(\"data\", \"tokenizer\", \"tokenizer.json\")\n",
    "\n",
    "# load tokenizer if it already exists\n",
    "if os.path.exists(tokenizer_path):\n",
    "    tokenizer = Structured(params=tokenizer_path)\n",
    "\n",
    "else:\n",
    "    # files to exclude:\n",
    "    idx_del = []\n",
    "    lookup = json.loads(lookup_path.read_text())\n",
    "    midi_paths_cleaned = [] \n",
    "    # files containing this string will be excluded\n",
    "    lookup_str = \"tuning\"\n",
    "    for key, title in lookup.items():\n",
    "        if lookup_str in title.lower():\n",
    "            idx_del.append(key)\n",
    "        # manually specify keys to exclude \n",
    "        elif str(key) in [\"0021\",\"0361\",\"0362\"]:   # verbal instructions and whole cds that couldnt be converted to midi (too long)\n",
    "            idx_del.append(key)\n",
    "        else:\n",
    "            midi_paths_cleaned.append(Path(\"data\", str(key)+\".mid\"))\n",
    "\n",
    "    print(f\"Excluded {len(idx_del)} files, {len(midi_paths_cleaned)} remaining.\")\n",
    "\n",
    "    # Builds the vocabulary with BPE\n",
    "    print(f\"Learning BPE...\")\n",
    "    tokenizer.learn_bpe(vocab_size=30000, files_paths=midi_paths_cleaned)\n",
    "    print(f\"Saving tokenizer to {tokenizer_path}\")\n",
    "    tokenizer.save_params(tokenizer_path)\n",
    "    print(\"Finished.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('data/0001.mid')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "midi_paths_cleaned[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<partitura.performance.Performance object at 0x7c685a302810>\n",
      "Number of notes in the MIDI file: 2188\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(219.03645, 3.1432292, 168220, 2414, 71, 28, 0, 0, 'n2187')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from partitura import load_performance_midi\n",
    "score = load_performance_midi(midi_paths_cleaned[0])\n",
    "# Get the duration of the MIDI file in seconds\n",
    "\n",
    "print(\"Number of notes in the MIDI file:\", len(score.note_array()))\n",
    "score.note_array()[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2559"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "midi = tokenizer(midi_paths_cleaned[0])\n",
    "len(midi[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader and Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: data: 100%|██████████| 1260/1260 [01:52<00:00, 11.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Tokenizing\")\n",
    "dataset = DatasetMIDI(\n",
    "    files_paths=midi_paths_cleaned,\n",
    "    min_seq_len=100,\n",
    "    max_seq_len=1024,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "collator = DataCollator(\n",
    "    tokenizer[\"PAD_None\"], tokenizer[\"BOS_None\"], tokenizer[\"EOS_None\"]\n",
    ")\n",
    "data_loader = DataLoader(dataset=dataset, collate_fn=collator)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Using the data loader in the training loop\n",
    "for i, batch in enumerate(tqdm(data_loader)):\n",
    "    print(f\"Training model on batch {i}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "practical",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
